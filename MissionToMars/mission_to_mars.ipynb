{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlMarsTable = './assets/pages/table.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Driver [/Users/stevewalker/.wdm/drivers/chromedriver/mac64/91.0.4472.19/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlRPS = 'https://redplanetscience.com'\n",
    "browser.visit(urlRPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "June 8, 2021\n",
      "Follow NASA's Perseverance Rover in Real Time on Its Way to Mars\n",
      "A crisply rendered web application can show you where the agency's Mars 2020 mission is right now as it makes its way to the Red Planet for a Feb. 18, 2021, landing.\n"
     ]
    }
   ],
   "source": [
    "# User BeautifulSoup to go to URL and get elements\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Error handling\n",
    "try:\n",
    "    # Identify and return news title\n",
    "    newsT = soup.find('div', class_='content_title').text\n",
    "    # Identify and return news paragraph\n",
    "    newsP = soup.find('div',class_='article_teaser_body').text\n",
    "    # Identify and return news date\n",
    "    newsD = soup.find('div',class_='list_date').text\n",
    "    \n",
    "\n",
    "    # Print results only if title and paragraph exist\n",
    "    if (newsT and newsP):\n",
    "        print('-------------')\n",
    "        print(newsD)\n",
    "        print(newsT)\n",
    "        print(newsP)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlSIM = 'https://spaceimages-mars.com'\n",
    "browser.visit(urlSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spaceimages-mars.com/image/featured/mars3.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get HTML\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Capture image information \n",
    "featureImageSrc = soup.find('img', class_='headerimage')['src']\n",
    "#featureImageSrc #sanitycheck\n",
    "\n",
    "# Capture image URL \n",
    "featureImageUrl = f\"{urlSIM}/{featureImageSrc}\"\n",
    "featureImageUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlGF = 'https://galaxyfacts-mars.com'\n",
    "\n",
    "# Load URL into pandas\n",
    "tables = pd.read_html(urlGF)\n",
    "\n",
    "#sanity checks\n",
    "#tables \n",
    "#type(tables) #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe table table striped table hover\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>Mars</th>      <th>Earth</th>    </tr>    <tr>      <th>Description</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Mars - Earth Comparison</th>      <td>Mars</td>      <td>Earth</td>    </tr>    <tr>      <th>Diameter:</th>      <td>6,779 km</td>      <td>12,742 km</td>    </tr>    <tr>      <th>Mass:</th>      <td>6.39 × 10^23 kg</td>      <td>5.97 × 10^24 kg</td>    </tr>    <tr>      <th>Moons:</th>      <td>2</td>      <td>1</td>    </tr>    <tr>      <th>Distance from Sun:</th>      <td>227,943,824 km</td>      <td>149,598,262 km</td>    </tr>    <tr>      <th>Length of Year:</th>      <td>687 Earth days</td>      <td>365.24 days</td>    </tr>    <tr>      <th>Temperature:</th>      <td>-87 to -5 °C</td>      <td>-88 to 58°C</td>    </tr>  </tbody></table>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df of the first table on the page\n",
    "df = tables[0]\n",
    "\n",
    "df=df.rename(columns={0:\"Description\",1:\"Mars\",2:\"Earth\"},errors=\"raise\")\n",
    "df.set_index(\"Description\",inplace=True)\n",
    "#df #sanity check    \n",
    "\n",
    "# convert df to html\n",
    "marsDataHtml = df.to_html(classes=['table','table striped','table hover']).replace('\\n','')\n",
    "marsDataHtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlMH = 'https://marshemispheres.com'\n",
    "browser.visit(urlMH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User BSoup to go to URL and get elements\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Create variable to hold the list of links\n",
    "items = soup.find_all('div', class_='item')\n",
    "#items #sanity check\n",
    "\n",
    "# Capture list of URLS for the pictures \n",
    "# by looping through tags\n",
    "urlList = []\n",
    "\n",
    "for item in items:\n",
    "    picDict = {} # going to pass in both the title and URL into dict\n",
    "    \n",
    "    picUrl = item.find('a')['href'] #the URL\n",
    "     \n",
    "    picTitle = item.find('h3').text\n",
    "    picTitle = picTitle.replace(' Enhanced','') #removed unneeded syntax\n",
    "    \n",
    "    picDict[picTitle]=f\"{urlMH}/{picUrl}\"\n",
    "    urlList.append(picDict)  #combines the parent URL to pic URL\n",
    "\n",
    "#urlList #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of pic URLS for the full resolution pics\n",
    "picsList = []\n",
    "\n",
    "for url in urlList:\n",
    "    #print(url) #sanity check\n",
    "    for key, value in url.items():\n",
    "        #print(key +\" - \" + value)\n",
    "        browser.visit(value)\n",
    "        \n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        picsDict = {}\n",
    "        \n",
    "        # Capture image information \n",
    "        imageSrc = soup.find('img', class_='wide-image')['src']\n",
    "        #print(ImageSrc) #sanitycheck\n",
    "\n",
    "        # Capture full image URL \n",
    "        imageUrl = f\"{urlMH}/{imageSrc}\"\n",
    "        #print(ImageUrl) #sanitycheck\n",
    "        \n",
    "        # Add above to dict\n",
    "        picsDict[\"title\"] = key\n",
    "        picsDict[\"img_url\"] = imageUrl\n",
    "        \n",
    "        # Add dict to list\n",
    "        picsList.append(picsDict)\n",
    "        \n",
    "picsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
