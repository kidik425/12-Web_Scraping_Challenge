{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlMarsTable = './assets/pages/table.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Driver [/Users/stevewalker/.wdm/drivers/chromedriver/mac64/91.0.4472.19/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlRPS = 'https://redplanetscience.com'\n",
    "browser.visit(urlRPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "June 7, 2021\n",
      "The Launch Is Approaching for NASA's Next Mars Rover, Perseverance\n",
      "The Red Planet's surface has been visited by eight NASA spacecraft. The ninth will be the first that includes a roundtrip ticket in its flight plan. \n"
     ]
    }
   ],
   "source": [
    "# User BeautifulSoup to go to URL and get elements\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Error handling\n",
    "try:\n",
    "    # Identify and return news title\n",
    "    newsT = soup.find('div', class_='content_title').text\n",
    "    # Identify and return news paragraph\n",
    "    newsP = soup.find('div',class_='article_teaser_body').text\n",
    "    # Identify and return news date\n",
    "    newsD = soup.find('div',class_='list_date').text\n",
    "    \n",
    "\n",
    "    # Print results only if title and paragraph exist\n",
    "    if (newsT and newsP):\n",
    "        print('-------------')\n",
    "        print(newsD)\n",
    "        print(newsT)\n",
    "        print(newsP)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlSIM = 'https://spaceimages-mars.com'\n",
    "browser.visit(urlSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spaceimages-mars.com/image/featured/mars2.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get HTML\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Capture image information \n",
    "featureImageSrc = soup.find('img', class_='headerimage')['src']\n",
    "#featureImageSrc #sanitycheck\n",
    "\n",
    "# Capture image URL \n",
    "featureImageUrl = f\"{urlSIM}/{featureImageSrc}\"\n",
    "featureImageUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlGF = 'https://galaxyfacts-mars.com'\n",
    "\n",
    "# Load URL into pandas\n",
    "tables = pd.read_html(urlGF)\n",
    "\n",
    "#sanity checks\n",
    "#tables \n",
    "#type(tables) #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df of the first table on the page\n",
    "df = tables[1]\n",
    "#df #sanity check\n",
    "\n",
    "# convert df to html\n",
    "df.to_html(htmlMarsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "urlMH = 'https://marshemispheres.com'\n",
    "browser.visit(urlMH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User BSoup to go to URL and get elements\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Create variable to hold the list of links\n",
    "items = soup.find_all('div', class_='item')\n",
    "#items #sanity check\n",
    "\n",
    "# Capture list of URLS for the pictures \n",
    "# by looping through tags\n",
    "urlList = []\n",
    "\n",
    "for item in items:\n",
    "    picDict = {} # going to pass in both the title and URL into dict\n",
    "    \n",
    "    picUrl = item.find('a')['href'] #the URL\n",
    "     \n",
    "    picTitle = item.find('h3').text\n",
    "    picTitle = picTitle.replace(' Enhanced','') #removed unneeded syntax\n",
    "    \n",
    "    picDict[picTitle]=f\"{urlMH}/{picUrl}\"\n",
    "    urlList.append(picDict)  #combines the parent URL to pic URL\n",
    "\n",
    "#urlList #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of pic URLS for the full resolution pics\n",
    "picsList = []\n",
    "\n",
    "for url in urlList:\n",
    "    #print(url) #sanity check\n",
    "    for key, value in url.items():\n",
    "        #print(key +\" - \" + value)\n",
    "        browser.visit(value)\n",
    "        \n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        picsDict = {}\n",
    "        \n",
    "        # Capture image information \n",
    "        imageSrc = soup.find('img', class_='wide-image')['src']\n",
    "        #print(ImageSrc) #sanitycheck\n",
    "\n",
    "        # Capture full image URL \n",
    "        imageUrl = f\"{urlMH}/{imageSrc}\"\n",
    "        #print(ImageUrl) #sanitycheck\n",
    "        \n",
    "        # Add above to dict\n",
    "        picsDict[\"title\"] = key\n",
    "        picsDict[\"img_url\"] = imageUrl\n",
    "        \n",
    "        # Add dict to list\n",
    "        picsList.append(picsDict)\n",
    "        \n",
    "picsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
